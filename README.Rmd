---
title: "The Analytics Edge - Kaggle Competition 2015"
author: "Jose A. Dianes"
date: "13 April 2015"
output:
  html_document:
    keep_md: yes
    theme: cosmo
    toc: yes
---

# Task description  

# Data loading and preparation  

```{r, message=FALSE}
library(tm)
library(ROCR)
library(rpart)
library(rpart.plot)
library(caTools)
library(randomForest)
library(caret)
library(e1071)
```

Let's start by reading the train and test data into the corresponding data frames.  

```{r}
newsTrain <- read.csv("data/NYTimesBlogTrain.csv", stringsAsFactors=FALSE)
newsTest <- read.csv("data/NYTimesBlogTest.csv", stringsAsFactors=FALSE)
summary(newsTrain)
```

From the summary we can see that we have several fields we could use to train
our models.  

# A simple bag-of-words model  

As a first approach, we will use bag-of-words models for teh headline text.  

## Preparing the corpus  

In order to build the corpus, we will go through the usual `tm` package calls.  

```{r}
corpusHeadline <- Corpus(VectorSource(c(newsTrain$Headline, newsTest$Headline)))
corpusHeadline <- tm_map(corpusHeadline, tolower)
corpusHeadline <- tm_map(corpusHeadline, PlainTextDocument)
corpusHeadline <- tm_map(corpusHeadline, removePunctuation)
corpusHeadline <- tm_map(corpusHeadline, removeWords, stopwords("english"))
corpusHeadline <- tm_map(corpusHeadline, stemDocument)
```

Now we are ready to convert our corpus to a DocumentTermMatrix, remove sparse 
terms, and turn it into a data frame. We selected one particular threshold to 
remove sparse terms. Later on we must try different ones.  

```{r}
dtm <- DocumentTermMatrix(corpusHeadline)
sparse <- removeSparseTerms(dtm, 0.99)
headlineWords <- as.data.frame(as.matrix(sparse))
```

Let's make sure our variable names are okay for R.  

```{r}
colnames(headlineWords) <- make.names(colnames(headlineWords))
```

## Training the models  

First we need to split the observations back into the training set and testing 
set. To do this, we can use the `head` and `tail` functions in `R`.  

```{r}
headlineWordsTrain <- head(headlineWords, nrow(newsTrain))
headlineWordsTest <- tail(headlineWords, nrow(newsTest))
```

Note that this split of HeadlineWords works to properly put the observations 
back into the training and testing sets, because of how we combined them 
together when we first made our corpus.  

Before building models, we want to add back the original variables from our
datasets. We'll add back the dependent variable to the training set, and the
`WordCount` variable to both datasets. Later on we will experiment with adding
more variables to use in our model.  

```{r}
headlineWordsTrain$Popular <- newsTrain$Popular
headlineWordsTrain$WordCount <- newsTrain$WordCount
headlineWordsTest$WordCount <- newsTest$WordCount
```

### Logistic regression  

Now let's create a logistic regression model using all of the variables.  

```{r}
headlineWordsLog <- glm(Popular ~ ., data=headlineWordsTrain, family=binomial)
```

And make predictions on our test set.  

```{r}
headlineWordsLogPredTest <- predict(headlineWordsLog, newdata=headlineWordsTest, type="response")
```

#### Evaluation  

In order to calculate the `AUC` we need the test data to be labeled with the final
response. But we don't have those labels. Actually we cannot evaluate any model
without those labels on the test data. In order to have some feedback from our
models performance we will split our training data into train and test and 
retrain/test the models using this new split.  

```{r}
spl <- sample.split(headlineWordsTrain$Popular, .7)
evalHeadlineWordsTrain <- subset(headlineWordsTrain, spl==T)
evalHeadlineWordsTest <- subset(headlineWordsTrain, spl==F)
```

Train the model.  

```{r}
evalHeadlineLogModel <- glm(Popular~., data=evalHeadlineWordsTrain, family=binomial)
```

Make predictions.  

```{r}
evalHeadlineLogPred <- predict(evalHeadlineLogModel, 
                               newdata=evalHeadlineWordsTest, type="response")
```

Get the `AUC` value.  

```{r}
headlineWordsLogROCR <- prediction(evalHeadlineLogPred, evalHeadlineWordsTest$Popular)
headlineWordsLogAUC <- as.numeric(performance(headlineWordsLogROCR, "auc")@y.values)
headlineWordsLogAUC
```

We obtain an `AUC` value slightly higher than that we get when submitting data
to Kaggle. We need to keep this in mind. But remember, *when making submissions
we will always use the complete training data to train our models and generate
the submission file*.  

#### Generating submission file  

Now we can prepare our submission file for Kaggle.  

```{r}
mySubmission = data.frame(UniqueID = newsTest$UniqueID, Probability1 = headlineWordsLogPredTest)
write.csv(mySubmission, "SubmissionHeadlineLog.csv", row.names=FALSE)
```

### CART  

As an alternative that will also give us some insight into the nature of the 
data, we will build now classification trees. The corpus and training/test
data are the same. We just need to train a different type model.  

```{r}
headlineWordsCARTModel <- rpart(Popular~., data=evalHeadlineWordsTrain, method="class")
prp(headlineWordsCARTModel)
```

Not a very effective model. Let us get just probabilities in order to see what
is happening.  

```{r}
headlineWordsCARTModel <- rpart(Popular~., data=evalHeadlineWordsTrain)
prp(headlineWordsCARTModel)
```

We see the problem. Probabilities are actually too small (lower than .5) so
the tree never predicts an article as popular using that threshold.  

Let's try now to make the tree more complex.  

```{r}
headlineWordsCARTModel <- rpart(Popular~., data=evalHeadlineWordsTrain, cp=0.0025)
prp(headlineWordsCARTModel)
```

Let's obtain predictions using this last model.  

```{r}
headlineWordsCARTPred <- predict(headlineWordsCARTModel, newdata=evalHeadlineWordsTest)
```

And let's evaluate this last model.  

```{r}
headlineWordsCARTROCR <- prediction(headlineWordsCARTPred, 
                                   evalHeadlineWordsTest$Popular)
headlineWordsCARTauc <- as.numeric(performance(headlineWordsCARTROCR, "auc")@y.values)
headlineWordsCARTauc
```

It doesn't improve the logistic regression model.  

### Random forest  

As a third method, we will try random forests.  

```{r}
healineWordsRF <- randomForest(Popular~., data=evalHeadlineWordsTrain)
```

Make predictions.  

```{r}
headlineWordsRFPred <- predict(healineWordsRF, newdata=evalHeadlineWordsTest)
```

Right into `auc` calculation.  

```{r}
headlineWordsRFrocr <- prediction(headlineWordsRFPred, evalHeadlineWordsTest$Popular)
headlineWordsRFauc <- as.numeric(performance(headlineWordsRFrocr, "auc")@y.values)
headlineWordsRFauc
```

#### Generating submission file  

This is the best model so far. Let's build it using the complete training set.  

```{r}
healineWordsRF <- randomForest(Popular~., data=headlineWordsTrain)
```

Make predictions.  

```{r}
headlineWordsRFPred <- predict(healineWordsRF, newdata=headlineWordsTest, type="response")
```

Now we can prepare our submission file for Kaggle.  

```{r}
mySubmission <- data.frame(
    UniqueID = newsTest$UniqueID, 
    Probability1 = abs(headlineWordsRFPred)
    )
write.csv(mySubmission, "SubmissionHeadlineRF.csv", row.names=FALSE)
```


# A richer model  

So far our model just used the term matrix and word counts. This is clearly
insuficient. Which of the other variables in the original dataset can make a
a difference? Actually we could classify those variables into:  

* Metadata variables, including `NewsDesk`, `SectionName`, `SubsectionName`,
`PubDate`.  
* Text variables, including `Headline`, `Snippet`, `Abstract`.  

In order to find out the first ones, we will build models using just them and
not data. In order to find out the text variables, we will create different 
corpora and repreat the initial approach. Finally we will combine them.  

## Logistic regression on metadata  

Let's build a new dataset including just the three factor variables and the date.  

```{r}
newsTrainMeta <- data.frame(
    Popular=newsTrain$Popular,
    NewsDesk = as.factor(newsTrain$NewsDesk),
    SectionName = as.factor(newsTrain$SectionName),
    SubsectionName = as.factor(newsTrain$SubsectionName)
    )
newsTestMeta <- data.frame(
    NewsDesk = factor(newsTest$NewsDesk, levels=levels(newsTrainMeta$NewsDesk)),
    SectionName = factor(newsTest$SectionName, levels=levels(newsTrainMeta$SectionName)),
    SubsectionName = factor(newsTest$SubsectionName, levels=levels(newsTrainMeta$Subsection))
    )
```

Now we build a linear model using all the variables.  

```{r}
metaLogModel <- glm(Popular~., data=newsTrainMeta, family=binomial)
summary(metaLogModel)
```

We see that some factor levels are meaningful, but not all of them. This might
be a good use case for classification trees and random forests.  

In order to evaluate our models, let's use the same split approach we used in
the bag of words section.  

```{r}
spl <- sample.split(newsTrainMeta$Popular, .75)
evalNewsTrain <- subset(newsTrainMeta, spl==T)
evalNewsTest <- subset(newsTrainMeta, spl==F)
```

Build again the model.  

```{r}
metaLogModel <- glm(Popular~., data=evalNewsTrain, family=binomial)
summary(metaLogModel)
```

Make predictions using the linear model.  

```{r}
metaLogPred <- predict(metaLogModel, newdata=evalNewsTest, type="response")
```

Get the `auc` value.  

```{r}
metaLogRocr <- prediction(metaLogPred, evalNewsTest$Popular)
metaLogAuc <- as.numeric(performance(metaLogRocr, "auc")@y.values)
metaLogAuc
```

We get an `auc` value of `r metaLogAuc`. The metadata by itself improves the 
text based model. This makes sense since most people have preference for specific
subjects and this makes them more popular.  

## Threes and Random Forests on metadata  

Randome forests are usually a good approach to classify non linear datasets like
the one we have based on metadata. But fist let's us try with a plain tree so we
can better understand what makes an article popular.  

```{r}
metaCartModel <- rpart(Popular~., data=evalNewsTrain, method="class")
prp(metaCartModel)
```

The three is clearly to coarse. It classifies popular articles as being just
part of a very specific grup of sections. Let's try with increasing the three
complexity, and also showing probabilities that are more expressive.  

```{r}
metaCartModel <- rpart(Popular~., data=evalNewsTrain, cp=.0005)
prp(metaCartModel)
```

Well, we have there a bunch of combinations of metadata that makes an article
popular these days.  

But let's evaluate this model using the `auc` value.  

```{r}
metaCartPred <- predict(metaCartModel, newdata=evalNewsTest)
metaCartRocr <- prediction(metaCartPred, evalNewsTest$Popular)
metaCartAuc <- as.numeric(performance(metaCartRocr, "auc")@y.values)
metaCartAuc
```

Not bad! We could try using cross-validation to find the right cp value.  


Define cross-validation experiment.  

```{r}
numFolds <- trainControl( method = "cv", number = 10 )
cpGrid <- expand.grid( .cp = seq(0.01,0.5,0.01)) 
```

Perform the cross validation.  

```{r}
tt <- train(Popular ~ ., data = evalNewsTrain, method = "rpart", trControl = numFolds, tuneGrid = cpGrid )
tt
```

Create a new CART model, predict, and evaluate.  

```{r}
metaCartCvModel <- rpart(Popular ~ ., data = evalNewsTrain, cp = 0.02)
metaCartCvPred <- predict(metaCartCvModel, newdata = evalNewsTest)
metaCartCvRocr <- prediction(metaCartCvPred, evalNewsTest$Popular)
metaCartCvAuc <- as.numeric(performance(metaCartCvRocr, "auc")@y.values)
metaCartCvAuc
```

We obtain an `auc` value for the bext `cp` of `r metaCartCvAuc`.  

Now with the random forest.  

```{r}
metaRfModel <- randomForest(Popular~., data=evalNewsTrain)
```

Do predictions.  

```{r}
metaRfPred <- predict(metaRfModel, newdata=evalNewsTest)
```

And the `auc` value.  

```{r}
metaRfRocr <- prediction(metaRfPred, evalNewsTest$Popular)
metaRfAuc <- as.numeric(performance(metaRfRocr, "auc")@y.values)
metaRfAuc
```

## Adding text to metadata  

Can we improve the accuracy of the metadata-based models by using our bag-of-words
and word count information? Let's start by creating a new dataframe with both
sources.  

```{r}
newsTrainMetaText <- cbind( newsTrainMeta, headlineWordsTrain)
newsTrainMetaText$Popular <- NULL # remove one of the Popular columns since they are replicated
```

Now we need to create again our train splits.  

```{r}
evalNewsMetaTextTrain <- subset(newsTrainMetaText, spl==T)
evalNewsMetaTextTest <- subset(newsTrainMetaText, spl==F)
```

### Logistic Regression  

Now we build the logistic regression model.  

```{r}
metaTextLogModel <- glm(Popular~., data=evalNewsMetaTextTrain, family=binomial)
summary(metaTextLogModel)
```

Evaluate the model.  

```{r}
metaTextLogPred <- predict(metaTextLogModel, newdata=evalNewsMetaTextTest, type="response")
metaTextLogRocr <- prediction(metaTextLogPred, evalNewsMetaTextTest$Popular)
metaTextLogAuc <- as.numeric(performance(metaTextLogRocr, "auc")@y.values)
metaTextLogAuc
```

That is a huge improvement but we should be careful because we had some warnings.  

### Classification Trees  

Let's try with the CART model. Again, we will find out the best `cp` value by
using corss-validation.  

```{r}
numFolds <- trainControl( method = "cv", number = 10 )
cpGrid <- expand.grid( .cp = seq(0.01,0.5,0.01)) 
tt <- train(Popular ~ ., data = evalNewsMetaTextTrain, method = "rpart", trControl = numFolds, tuneGrid = cpGrid )
tt
```

This time we get `cp=0.01`. Let's build the model.  

```{r}
metaTextCartCvModel <- rpart(Popular~., data=evalNewsMetaTextTrain, cp=0.01)
metaTextCartCvPred <- predict(metaTextCartCvModel, newdata = evalNewsMetaTextTest)
metaTextCartCvRocr <- prediction(metaTextCartCvPred, evalNewsMetaTextTest$Popular)
metaTextCartCvAuc <- as.numeric(performance(metaTextCartCvRocr, "auc")@y.values)
metaTextCartCvAuc
```

And finally random forests.  

```{r}
metaTextRfModel <- randomForest(Popular~., data=evalNewsMetaTextTrain)
metaTextRfPred <- predict(metaTextRfModel, newdata=evalNewsMetaTextTest)
metaTextRfRocr <- prediction(metaTextRfPred, evalNewsMetaTextTest$Popular)
metaTextRfAuc <- as.numeric(performance(metaTextRfRocr, "auc")@y.values)
metaTextRfAuc
```

That is a really high `auc`. Let's save our results but first train the model
using the complete training set.  

```{r}
metaTextRfModel <- randomForest(Popular~., data=newsTrainMetaText)
newsTestMetaText <- cbind(newsTestMeta, headlineWordsTest)
metaTextRfPred <- predict(metaTextRfModel, newdata=newsTestMetaText)
mySubmission <- data.frame(
    UniqueID = newsTest$UniqueID, 
    Probability1 = abs(metaTextRfPred)
    )
write.csv(mySubmission, "SubmissionMetaTextRF.csv", row.names=FALSE)
```

### A look at difficult cases  

Let's check what is in those cases that our finest model (`metaTextRfModel`)
didn't classify properly. This will give us some clues about new predictors
or ways to approach the problem.  

# About dates  

Maybe an article date is not so important, but what about the day of the week
or the month of the year? Certain periods are more likely to have readers than 
others.  


# Cross-validation on Random Forests  


# TF-IDF  

By removing sparseness based just on counts we might be removing some important
words. A better approach is using TF-IDF.  

# Clustering    

- By clustering on the corpus and then table for Popular, we will extract
words that make an article popular?  
- Cluster then predict  


# Experiment: using separate models for each section and average them manually with different weights   

Rational behind this?  



